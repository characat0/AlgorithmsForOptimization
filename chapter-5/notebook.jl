### A Pluto.jl notebook ###
# v0.12.4

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# â•”â•â•¡ 839a6840-13f2-11eb-3ee7-3b513eb25cac
using Calculus

# â•”â•â•¡ 3a625020-13fc-11eb-3cbb-95bafc7c95f9
using Base.MathConstants

# â•”â•â•¡ febfa170-13fc-11eb-3291-f9bdc4e9f971
using Plots

# â•”â•â•¡ dd193610-13fe-11eb-1724-77e60a1eb306
using PlutoUI

# â•”â•â•¡ 49266a30-1422-11eb-1d84-9d24a1b81e04
using LinearAlgebra

# â•”â•â•¡ 8bd329d0-13f1-11eb-0e83-25a8a9620553
md"# First-Order Methods"

# â•”â•â•¡ 68f68ae0-1510-11eb-0bb6-bb84b68d18e6
md"### Spoiler alert: Some methods are trash!"

# â•”â•â•¡ a97881d0-150c-11eb-3391-b52524901b4c
begin
	fâ‚™(x, y)::Number = (1-x)^2 + 100(y - x^2)^2
	fâ‚™(xÌ„)::Number = fâ‚™(xÌ„...)
	âˆ‡fâ‚™(xÌ„) = Calculus.gradient(fâ‚™, xÌ„)
end;

# â•”â•â•¡ c71545d0-154c-11eb-38d2-ebdde946ae88
begin
	xâ‚™ = range(-1, 3, length=200)
	yâ‚™ = range(-1, 2, length=200)
end;

# â•”â•â•¡ fbe594ce-150c-11eb-3135-fb523622c0cd
@bind nâ‚™ Slider(1:200, default=12, show_value=true)

# â•”â•â•¡ fbc03280-150c-11eb-3400-cf19d15006ba
@bind xâ‚™áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ fb16ac10-150c-11eb-23af-816bfbad1ad9
@bind yâ‚™áµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ 9cf0b980-13f1-11eb-2274-6bc775d469c8
md"## Gradient Descent"

# â•”â•â•¡ cb1de700-13f2-11eb-2ace-afc2e372b013
md"We can choose the direction of steepest descent as the descent direction d, and this is the opposite of the gradient."

# â•”â•â•¡ ea5c8f40-13f2-11eb-22b5-61728c0e4388
md"\$\$d^{(k)}=-\frac{g^{(k)}}{||g^{(k)}||}\$\$"

# â•”â•â•¡ 290a8cb0-13f3-11eb-2069-4f8e02d036a8
md"It is proven (proof left to the reader as excercise) that each direction is orthogonal to the previous."

# â•”â•â•¡ bcfb7970-13f3-11eb-3e89-d7e04bfc65ad
function fibonacci_search(f::Function, a::Number, b::Number, n::Integer; Ïµ=0.01)::Number
	s = (1-âˆš5)/(1+âˆš5)
	p = 1/(Ï†*(1-s^(n+1))/(1-s^n))
	d = p*b + (1-p)*a
	yd = f(d)
	for i in 1:n-1
		if 1 == n-1
			c = Ïµ*a + (1-Ïµ)*d
		else
			c = p*a + (1-p)*b
		end
		yc = f(c)
		if yc < yd
			b, d, yd = d, c, yc
		else
			a, b = b, c
		end
		p = 1/(Ï†*(1-s^(n-i+1))/(1-s^(n-i)))
	end
	(a+b)/2
end

# â•”â•â•¡ 58ee91f0-13f4-11eb-1d27-31633b18d5ee
function bracket_minimum(f::Function, x=0.0; s=1e-12, k=2.0)
	try
		a, ya = x, f(x)
		global b, yb = x+s, f(x+s)
		if yb > ya #If we are not going downhill
			c, yc = x-s, f(x-s)
			s = -s
		else
			c, yc = x+k*s, f(x+k*s)	
		end
		if ya > yb < yc # Definition of unimodal
			return (a, c)
		end
		s = s*k
	catch e
		println(e)
		return (a, c)
	end
	return bracket_minimum(f, b; s=s, k=k)
end

# â•”â•â•¡ 246f8b50-13f4-11eb-2e29-977facb5c488
minimize(f::Function, a::Number, b::Number)::Number = fibonacci_search(f, a, b, 5)

# â•”â•â•¡ 33062200-13f4-11eb-21f1-1ffa95638977
function optimize_line_search(f, x, d)::Float64
	objective(Î±::Number)::Number = f(x + Î±*d)
	a, b = bracket_minimum(objective; k=1.05)
	Î± = minimize(objective, a, b)
	Î±
end

# â•”â•â•¡ d330cce2-13f8-11eb-2d7b-eb0338c3d1b8
function gradient_descent_mixed(f, âˆ‡f, x, k)::Array{Tuple, 1}
	points::Array{Tuple} = Array{Tuple}(undef, k+1)
	points[1] = tuple(x...)
	for i in 1:k
		d = -âˆ‡f(x)
		d = d/sqrt(sum(2 .^ d))
		Î± = optimize_line_search(f, x, d)
		x = x + Î±*d
		points[i+1] = tuple(x...)
	end
	points
end

# â•”â•â•¡ dcdfefe0-13f9-11eb-337a-2d8a80e4a5e7
begin
	fâ‚(x, y)::Number = (1-x)^2 + 5(y - x^2)^2
	fâ‚(xÌ„)::Number = fâ‚(xÌ„...)
end

# â•”â•â•¡ ecdf9a32-13fe-11eb-3ff9-5166ac683584
@bind nâ‚ Slider(1:50, default=20, show_value=true)

# â•”â•â•¡ 6dc753e0-13ff-11eb-01ae-7d4477726252
@bind xâ‚áµ¢ Slider(-2.5:.01:2.5, default=-0.96, show_value=true)

# â•”â•â•¡ 9841a800-13ff-11eb-1ef2-0b366a3d3b22
@bind yâ‚áµ¢ Slider(-2.5:.01:2.5, default=1.69, show_value=true)

# â•”â•â•¡ 01b03910-13fa-11eb-38fe-7ff1876faefc
begin
	xâ‚ = range(-3, stop=3, length=200)
	yâ‚ = xâ‚
	colorâ‚ = cgrad([:grey, :blue])
	âˆ‡fâ‚(xÌ„) = Calculus.gradient(fâ‚, xÌ„)
	pointsâ‚ = [x for x in gradient_descent_mixed(fâ‚, âˆ‡fâ‚, [xâ‚áµ¢, yâ‚áµ¢], nâ‚)]
	plotâ‚ = contour(xâ‚, yâ‚, fâ‚, c=colorâ‚, levels=700 .^ (range(-.2,stop=1,length=14)))
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚áµ¢, yâ‚áµ¢), markersize=2, c=:black, label="Initial point")
	plot!(pointsâ‚, c=:red, label="Descent")
	plotâ‚
end

# â•”â•â•¡ 8107d7b0-140c-11eb-0194-67b53124e6a2
md"The optimal point is near _$(pointsâ‚[end])_"

# â•”â•â•¡ 965666e0-140c-11eb-3d64-6fea80cb3edc
md"The optimized value is _$(fâ‚(pointsâ‚[end]))_"

# â•”â•â•¡ 8116c470-140f-11eb-1b12-230e8c180dfe
abstract type DescentMethod end

# â•”â•â•¡ 2fdb8310-1401-11eb-1529-ed74dd276380
begin
	struct GradientDescent <: DescentMethod
		Î±
	end
end

# â•”â•â•¡ a8ae43ee-140f-11eb-2adf-4f4e1d0a8ef5
init!(M::GradientDescent, f, âˆ‡f, x) = M

# â•”â•â•¡ 7cc17412-140f-11eb-0411-4f7deaa61fae
function step!(M::GradientDescent, f, âˆ‡f, x)::Array{Float64, 1}
	Î±, g = M.Î±, âˆ‡f(x)
	return x - Î±*g
end

# â•”â•â•¡ bd87d4c2-1410-11eb-3a2c-07be02f2d049
fâ‚‚ = fâ‚

# â•”â•â•¡ df37f230-1410-11eb-0593-d5008a92eae1
@bind nâ‚‚ Slider(1:100, default=20, show_value=true)

# â•”â•â•¡ dea91fb0-1410-11eb-2c52-836b5e46eb28
@bind xâ‚‚áµ¢ Slider(-2.5:.01:2.5, default=-0.96, show_value=true)

# â•”â•â•¡ de622ba0-1410-11eb-2910-b1e14a97878f
@bind yâ‚‚áµ¢ Slider(-2.5:.01:2.5, default=1.69, show_value=true)

# â•”â•â•¡ fa788512-1400-11eb-286c-85633d52af21
md"Gradient descent (alone) is not miraculous ğŸ˜¢"

# â•”â•â•¡ d1732fa0-13fe-11eb-39f0-670f5eaa94ec
md"## Conjugate Gradient"

# â•”â•â•¡ 4d0090c0-141a-11eb-3bbe-d9049235aca1
md"Gradient descent is ğŸ—‘ï¸ in narrow valleys. This method uses a quadratic approximation of the local function to find optimal points."

# â•”â•â•¡ 52541a70-141e-11eb-2096-c75cb6bfd792
md"Conjugate Gradient Descent is GUARANTEED to optimize a n-dimensional quadratic function in n steps."

# â•”â•â•¡ 5d834dc0-141f-11eb-25eb-63933ff6ae4c
md"This method can be applied to nonquadratic functions. Smooth functions behave quadratic close to a local minimum (oh-oh)."

# â•”â•â•¡ 0bbda8e0-1420-11eb-25bc-c34eaf97c1b6
md"This means that we need to get close to the minimum in order to obtain benefits. ğŸ—‘ï¸"

# â•”â•â•¡ 2418b4c0-1420-11eb-1d36-b5c012477a99
md"This method uses information from last gradients to improve:"

# â•”â•â•¡ 75156ee0-1420-11eb-21ba-71bbcff2a1ed
md"\$\$d^{(k+1)}=-g^{(k+1)} + Î²^{(k)}d^{(k)}\$\$"

# â•”â•â•¡ 9a938440-1420-11eb-19f5-91f76c2eab12
md"**Fletcher-Reeves:**\$\$Î²^{(k)}=\frac{(g^{(k)})^2}{(g^{(k-1)})^2}\$\$"

# â•”â•â•¡ 0d96b92e-1421-11eb-11f0-6dc604eeb10a
md"**Polak-Ribiere:**\$\$Î²^{(k)}=\frac{g^{(k)T}(g^{(k)}-g^{(k-1)})}{(g^{(k-1)})^2}\$\$"

# â•”â•â•¡ 40b8c7e0-1421-11eb-3e76-5db9f1885668
md"ğŸ‘€ For guaranteed convergence on the Polak-Ribiere method we need to reset Î² to 0 if it goes negative."

# â•”â•â•¡ 744aaf60-1421-11eb-0dbd-1fd4f4775c8d
begin
	mutable struct ConjugateGradientDescent <: DescentMethod
		d::Array{Number, 1}
		g::Array{Number, 1}
	end
	function ConjugateGradientDescent()
		return ConjugateGradientDescent(Number[], Number[])
	end
end

# â•”â•â•¡ 08462370-1422-11eb-1ea0-ef8e58d85970
function init!(M::ConjugateGradientDescent, f, âˆ‡f, x)
	M.g = âˆ‡f(x)
	M.d = -M.g
	return M
end

# â•”â•â•¡ 98c33ba0-1421-11eb-23ce-a76943bc3a76
function step!(M::ConjugateGradientDescent, f, âˆ‡f, x)::Array{Float64, 1}
	d, g = M.d, M.g
	gâ€² = âˆ‡f(x)
	Î² = (gâ€² â‹… (gâ€² - g))/(g â‹… g)
	Î² = Î²*(Î²>0)
	dâ€² = -gâ€² + Î²*d
	Î± = optimize_line_search(f, x, dâ€²)
	xâ€² = x + Î±*dâ€²
	M.d, M.g = dâ€², gâ€²
	return xâ€²
end

# â•”â•â•¡ 8accdfc0-1425-11eb-18fb-29f87b39e006
fâ‚ƒ = fâ‚

# â•”â•â•¡ 6bcaf260-1425-11eb-1b6a-b14640cd9d64
@bind nâ‚ƒ Slider(1:10, default=5, show_value=true)

# â•”â•â•¡ 7163e650-1425-11eb-2bb9-6b6b8c3bf71a
@bind xâ‚ƒáµ¢ Slider(-2.5:.01:2.5, default=-0.96, show_value=true)

# â•”â•â•¡ 7ac3cd50-1425-11eb-1a93-adaeb3dac3f4
@bind yâ‚ƒáµ¢ Slider(-2.5:.01:2.5, default=1.69, show_value=true)

# â•”â•â•¡ 7d0d8450-1427-11eb-373d-fd77eca043b3
md"## Momentum"

# â•”â•â•¡ b1426290-1427-11eb-3ff3-4d936d73df78
md"The idea is to simulate a force (that updates velocity) to avoid slow movement in a nearly flat surface (gradients with small magnitude will fail miserably)."

# â•”â•â•¡ d250f360-1428-11eb-3067-7bf3b10eeb95
md"\$\$v^{(k+1)}=Î²v^{(k)}-Î±g^{(k)}\$\$"

# â•”â•â•¡ bf228510-1428-11eb-35e0-ef99d695c0de
md"\$\$x^{(k+1)}=x^{(k)}+v^{(k+1)}\$\$"

# â•”â•â•¡ ff655350-1428-11eb-2baf-079bacea6696
begin
	mutable struct Momentum <: DescentMethod
		Î±::Number
		Î²::Number
		v::Array{Number, 1}
	end
	Momentum(Î±, Î²) = Momentum(Î±, Î², Number[])
end

# â•”â•â•¡ b9c8eea0-1429-11eb-315c-316fc17a30a1
function init!(M::Momentum, f, âˆ‡f, x)
	M.v = zeros(length(x))
end

# â•”â•â•¡ 0c382480-142a-11eb-026a-77a06e8eb9d0
function step!(M::Momentum, f, âˆ‡f, x)::Array{Float64, 1}
	Î±, Î², v, g = M.Î±, M.Î², M.v, âˆ‡f(x)
	v[:] = Î²*v - Î±*g #Update velocity
	return x + v
end

# â•”â•â•¡ 87a82de0-142a-11eb-3c40-3f50754cc0fe
begin
	fâ‚„(x, y)::Number = (1-x)^2 + 100(y - x^2)^2
	fâ‚„(xÌ„)::Number = fâ‚„(xÌ„...)
end

# â•”â•â•¡ 1b6bfab0-142c-11eb-1d7e-995795b8d26f
@bind nâ‚„ Slider(1:40, default=21, show_value=true)

# â•”â•â•¡ 2279db60-142c-11eb-1d5d-f16fbfdf7451
@bind xâ‚„áµ¢ Slider(-1.5:.01:1.5, default=-0.58, show_value=true)

# â•”â•â•¡ 23190190-142c-11eb-2cc2-614ee78d3e8a
@bind yâ‚„áµ¢ Slider(0:.015:1.5, default=0.36, show_value=true)

# â•”â•â•¡ 349bc640-142d-11eb-3baa-e7dc46b4b8a1
md"It's like the point is falling straight to the minimum ğŸ˜²"

# â•”â•â•¡ 76d33890-142d-11eb-253a-491b80114239
md"But... it might go too fast"

# â•”â•â•¡ a38f6660-142d-11eb-21a2-8529e11f8acb
md"## Nesterov Momentum"

# â•”â•â•¡ 9b6ad3a0-142f-11eb-3482-ed377d4bb2a8
md"If it goes too fast try predicting how fast it will go the next iteration..."

# â•”â•â•¡ 72a17620-1477-11eb-166a-fd7dac2a01bd
md"\$\$v^{(k+1)}=Î²v^{(k)}-Î±âˆ‡f(x^{(k)}+Î²v^{(k)})\$\$"

# â•”â•â•¡ 93479da0-1477-11eb-0c7b-a9446b9dd6d3
begin
	mutable struct NesterovMomentum <: DescentMethod
		Î±::Number
		Î²::Number
		v::Array{Number, 1}
	end
	function NesterovMomentum(Î±::Number, Î²::Number)
		NesterovMomentum(Î±, Î², Number[])
	end
	function init!(M::NesterovMomentum, f, âˆ‡f, x)
		M.v = zeros(length(x))
		return M
	end
	function step!(M::NesterovMomentum, f, âˆ‡f, x)::Array{Float64, 1}
		Î±, Î², v = M.Î±, M.Î², M.v
		next_g = Î²*v
		v[:] = next_g - Î±*âˆ‡f(x + next_g)
		return x + v
	end
end

# â•”â•â•¡ f8d6b0f0-1479-11eb-1c27-4b98bb05bd34
fâ‚… = fâ‚„

# â•”â•â•¡ fd7f9f92-1479-11eb-11de-c9b923f3fe06
@bind nâ‚… Slider(1:1000, default=42, show_value=true)

# â•”â•â•¡ 0424d360-147a-11eb-2b4a-99845e4f9e5a
@bind xâ‚…áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ 0bc30d30-147a-11eb-2480-53595b44dd79
@bind yâ‚…áµ¢ Slider(-1.5:.01:1.5, default=1.42, show_value=true)

# â•”â•â•¡ db2fd4d0-147b-11eb-35a8-770858111cba
md"It's still going too fast..."

# â•”â•â•¡ 0c426470-147c-11eb-1e91-f131c1a1da0b
md"## Adagrad"

# â•”â•â•¡ ad146fb0-147c-11eb-3756-33ad44898b84
md"Adaptative subgradient method"

# â•”â•â•¡ cbecf130-147e-11eb-0748-0d4f4af1775c
md"This method updates learning rate for each component."

# â•”â•â•¡ e572c170-147e-11eb-0931-c5485d3e4203
md"\$\$x\_i ^{(k+1)}=x\_i ^{(k)} - \frac{Î±}{Ïµ+\sqrt{s\_i ^{(k)}}}g\_i ^{(k)}\$\$"

# â•”â•â•¡ 349f9d90-147f-11eb-3868-8b295d0f4e59
md"\$\$ s\_i ^{(k)} = \sum\_{j=1}^{k} (g\_i ^{(j)})^2 \$\$"

# â•”â•â•¡ 49aaba22-148a-11eb-3782-cd3dcb9a53b1
md"Ïµ is a small value (1e-8) to prevent division by zero."

# â•”â•â•¡ 6a12c730-148a-11eb-3fa7-63cf835a531d
md"Î± is usually set to .01, because of all the operations it doesn't matter too much"

# â•”â•â•¡ 7d457b40-148a-11eb-101a-15c60623e966
md"Problem: As we can see, the components of S grow over time, making the learning rate decrease (and become infinitesimally small even before convergence)."

# â•”â•â•¡ af9daa40-148a-11eb-0b19-452c71a11b26
begin
	mutable struct Adagrad <: DescentMethod
		Î±::Number
		Ïµ::Number
		s::Array{Float64, 1}
	end
	Adagrad(Î±::Number, Ïµ::Number) = Adagrad(Î±, Ïµ, Float64[])
	Adagrad(Î±::Number) = Adagrad(Î±, 1e-8)
	function init!(M::Adagrad, f, âˆ‡f, x)
		M.s = zeros(length(x))
		return M
	end
	function step!(M::Adagrad, f, âˆ‡f, x)::Array{Float64, 1}
		Î±, Ïµ, s, g = M.Î±, M.Ïµ, M.s, âˆ‡f(x)
		s[:] += g .* g
		return x - Î±*g ./ (sqrt.(s) .+ Ïµ)
	end
end

# â•”â•â•¡ e6fe3080-148b-11eb-28ba-8f5a1b03e5cb
fâ‚† = fâ‚„

# â•”â•â•¡ f62399b0-148b-11eb-2078-c97839d84aaf
@bind nâ‚† Slider(1:100, default=22, show_value=true)

# â•”â•â•¡ fb8c07c0-148b-11eb-3291-f985249cb0ae
@bind xâ‚†áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ 00b019d0-148c-11eb-0c2b-bb5287624389
@bind yâ‚†áµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ cf5ede10-148c-11eb-2fe9-ddedbf608aad
md"Really slow convergence..."

# â•”â•â•¡ c6ffcfd2-148d-11eb-3486-1fd4742afaa0
md"## RMSProp"

# â•”â•â•¡ e83f7830-148d-11eb-0119-770094c7ad55
md"It comes to the rescue of Adagrad, solving the problem of decreasing learning rate."

# â•”â•â•¡ 40e2a980-148e-11eb-1076-5767cba6fff6
md"âŠ™ = \odot (element wise multiplication)"

# â•”â•â•¡ fc7607ae-148d-11eb-25fa-59a3d2c4c722
md"\$\$sÌ‚^{(k+1)}=Î³sÌ‚^{(k)}+(1-Î³)(g^{(k)}âŠ™g^{(k)})\$\$"

# â•”â•â•¡ e5daad30-148d-11eb-224a-1bb3c3d5b20a
md"Î³ is tipically close to 0.9"

# â•”â•â•¡ 6595a6b0-148e-11eb-3a6a-8dfdb2ef7933
md"\$\$RMS(g\_i)=sÌ‚\_i ^{(k+1)}\$\$"

# â•”â•â•¡ 65ad4d5e-148e-11eb-1121-c139c1000455
begin
	mutable struct RMSProp <: DescentMethod
		Î±::Number
		Î³::Number
		Ïµ::Number
		sÌ‚::Array{Float64, 1}
	end
	RMSProp(Î±::Number, Î³::Number, Ïµ::Number) = RMSProp(Î±, Î³, Ïµ, Float64[])
	RMSProp(Î±::Number, Î³::Number) = RMSProp(Î±, Î³, 1e-8)
	RMSProp(Î±::Number) = RMSProp(Î±, .9)
	function init!(M::RMSProp, f, âˆ‡f, x)
		M.sÌ‚ = zeros(length(x))
		return M
	end
	function step!(M::RMSProp, f, âˆ‡f, x)
		Î±, Î³, Ïµ, sÌ‚, g = M.Î±, M.Î³, M.Ïµ, M.sÌ‚, âˆ‡f(x)
		sÌ‚[:] = Î³*sÌ‚ + (1-Î³)*(g .* g)
		return x - Î±*g ./ (sqrt.(sÌ‚) .+ Ïµ)
	end
end

# â•”â•â•¡ f373bf20-148f-11eb-178b-ddd8ed8df869
fâ‚‡ = fâ‚„

# â•”â•â•¡ f71a8000-148f-11eb-08bb-911cfaa79535
@bind nâ‚‡ Slider(1:300, default=22, show_value=true)

# â•”â•â•¡ f87c8100-148f-11eb-3938-2dd7affdd0a4
@bind xâ‚‡áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ f8981f50-148f-11eb-1b9b-3dfd2d711494
@bind yâ‚‡áµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ 0f715b40-1492-11eb-0c2f-4d5ff35414b6
md"## Adadelta"

# â•”â•â•¡ 17b8a650-1492-11eb-3fcb-9dce6ad8e093
md"Removes the learning rate parameter entirely (the same authors of RMSProp did this) ğŸ˜²ğŸ˜²"

# â•”â•â•¡ 0ceaa960-14df-11eb-3e04-613396d49c39
md"\$\$x\_i ^{(k+1)}=x\_i ^{(k)} - \frac{RMS(Î”x\_i)}{Ïµ+RMS(g\_i)}\$\$"

# â•”â•â•¡ 35045130-14df-11eb-227e-af37a738cf78
begin
	mutable struct Adadelta <: DescentMethod
		Î³s::Number
		Î³x::Number
		Ïµ::Number
		sÌ‚::Array{Float64, 1}
		u::Array{Float64, 1}
	end
	Adadelta(Î³s::Number, Î³x::Number, Ïµ::Number) = Adadelta(Î³s, Î³x, Ïµ, Float64[], Float64[])
	Adadelta(Î³s::Number, Î³x::Number) = Adadelta(Î³s, Î³x, 1e-2)
	function init!(M::Adadelta, f, âˆ‡f, x)
		M.sÌ‚ = zeros(length(x))
		M.u = zeros(length(x))
		return M
	end
	function step!(M::Adadelta, f, âˆ‡f, x)::Array{Float64, 1}
		Î³s, Î³x, Ïµ, sÌ‚, u, g = M.Î³s, M.Î³x, M.Ïµ, M.sÌ‚, M.u, âˆ‡f(x)
		sÌ‚[:] = Î³s*sÌ‚ + (1-Î³s)*(g .* g)
		Î”x = -(sqrt.(u) .+ Ïµ) ./ (sqrt.(sÌ‚) .+ Ïµ) .* g
		u[:] = Î³x*u + (1-Î³x)*(Î”x .* Î”x)
		return x + Î”x
	end
end

# â•”â•â•¡ 8ec902a0-14e0-11eb-0cba-f1917748a92f
fâ‚ˆ = fâ‚„

# â•”â•â•¡ b429cc00-14e0-11eb-27ba-b76917406f92
@bind nâ‚ˆ Slider(1:1000, default=102, show_value=true)

# â•”â•â•¡ b3fc0540-14e0-11eb-1975-47351cf57db3
@bind xâ‚ˆáµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ b37dfba0-14e0-11eb-161e-812bbfff5a45
@bind yâ‚ˆáµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ 22447bd0-14e2-11eb-2385-afe72e00c514
md"## Adam"

# â•”â•â•¡ 8d785802-14e6-11eb-2768-7d57bb0c552d
md"Adaptative moment estimation method"

# â•”â•â•¡ 80d5c920-14e6-11eb-0b2b-aba1f51d4c40
md"Stores exponentially decaying squared gradient (RMSProp and Adadelta), but also decaying gradient like momentum."

# â•”â•â•¡ f4919fb0-14e6-11eb-37da-7f8b001f912f
md"Initializing the gradient and squared gradient to zero introduces a bias. (Good defaults are Î±=.001, Î³áµ¥=.9, Î³â‚›=.999 and Ïµ=1e-8)"

# â•”â•â•¡ 53fc59e0-14e7-11eb-1e42-c93841a25d8b
md"The equations for Adam are:"

# â•”â•â•¡ 751b81f0-14e7-11eb-30dd-d914bd2aae92
md"- Biased decaying momentum: \$\$ v^{(k+1)} = Î³\_v v^{(k)} + (1-Î³\_v)g^{(k)}\$\$"

# â•”â•â•¡ 75af8490-14e7-11eb-266b-2f84b10c59b6
md"- Biased decaying sq. gradient: \$\$s^{(k+1)} = Î³\_ss^{(k)}+(1-Î³\_s)(g^{(k)}âŠ™g^{(k)})\$\$"

# â•”â•â•¡ 75cb9810-14e7-11eb-0a01-15c658df3cd6
md"- Corrected decaying momentum: \$\$vÌ‚^{(k+1)}=v^{(k+1)}/(1-Î³\_v ^k)\$\$"

# â•”â•â•¡ 75e93230-14e7-11eb-1436-155363e20db8
md"- Corrected decaying sq. gradient: \$\$sÌ‚^{(k+1)} = s^{(k+1)}/(1-Î³\_s ^k)\$\$"

# â•”â•â•¡ a6b66980-14e9-11eb-1f40-4b6f60bd56db
md"- Next iterate: \$\$x^{(k+1)}=x^{(k)}-Î±vÌ‚^{(k+1)}/(Ïµ+\sqrt{sÌ‚^{(k+1)}})\$\$"

# â•”â•â•¡ c9666ac2-14e9-11eb-3f14-cbf7adf802d9
begin
	mutable struct Adam <: DescentMethod
		Î±::Float64
		Î³áµ¥::Float64
		Î³â‚›::Float64
		Ïµ::Float64
		k::Integer
		v::Array{Float64, 1}
		s::Array{Float64, 1}
	end
	Adam(Î±::Number, Î³áµ¥::Number, Î³â‚›::Number, Ïµ::Number) = Adam(Î±, Î³áµ¥, Î³â‚›, Ïµ, 0, Float64[], Float64[])
	Adam(Î±::Number, Î³áµ¥::Number, Î³â‚›::Number) = Adam(Î±, Î³áµ¥, Î³â‚›, 1e-8)
	Adam(Î±::Number, Î³áµ¥::Number) = Adam(Î±, Î³áµ¥, .999)
	Adam(Î±::Number) = Adam(Î±, .9)
	
	function init!(M::Adam, f, âˆ‡f, x)
		M.k = 0
		M.v = zeros(length(x))
		M.s = zeros(length(x))
		return M
	end
	
	function step!(M::Adam, f, âˆ‡f, x)
		Î±, Î³áµ¥, Î³â‚›, Ïµ, k = M.Î±, M.Î³áµ¥, M.Î³â‚›, M.Ïµ, M.k
		s, v, g = M.s, M.v, âˆ‡f(x)
		v[:] = Î³áµ¥*v + (1-Î³áµ¥)*g
		s[:] = Î³â‚›*s + (1-Î³â‚›)*(g .* g)
		M.k = k += 1 #Updates k at the same time
		vÌ‚ = v/(1-Î³áµ¥^k)
		sÌ‚ = s/(1-Î³â‚›^k)
		return x - (Î±*vÌ‚)./(sqrt.(sÌ‚) .+ Ïµ)
	end
end

# â•”â•â•¡ cf904cb0-14ec-11eb-3cec-af99445c4bb3
fâ‚‰ = fâ‚„

# â•”â•â•¡ 3955ac20-14ee-11eb-314d-ab74464eb64b
@bind nâ‚‰ Slider(1:500, default=222, show_value=true)

# â•”â•â•¡ 393c57c0-14ee-11eb-09fd-a51eb1d06292
@bind xâ‚‰áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ 39257460-14ee-11eb-365d-45eca91e680e
@bind yâ‚‰áµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ 94087c30-14f1-11eb-2df8-27240ebdd2ae
md"## Hypergradient Descent"

# â•”â•â•¡ 3b6a5e82-14f2-11eb-1962-d19f98382b7a
md"This methods are too sensitive to the learning rate... let's optimize it first"

# â•”â•â•¡ 5fd33350-14f2-11eb-0548-7db92e9c8551
md"We applied gradient descent to the learning reate ğŸ¤¯ğŸ¤¯"

# â•”â•â•¡ 59a20190-14f3-11eb-3b9b-7d4284e3f759
begin
	mutable struct HyperGradientDescent <: DescentMethod
		Î±â‚€::Float64 #Initial learning rate
		Î¼::Float64 #Inception learning rate
		Î±::Float64 #Current learning rate
		g_prev::Array{Float64, 1}
	end
	
	HyperGradientDescent(Î±â‚€::Number, Î¼::Number) = HyperGradientDescent(Î±â‚€, Î¼, Î±â‚€, Float64[])
	
	function init!(M::HyperGradientDescent, f, âˆ‡f, x)
		M.Î± = M.Î±â‚€
		M.g_prev = zeros(length(x))
		return M
	end
	
	function step!(M::HyperGradientDescent, f, âˆ‡f, x)::Array{Float64, 1}
		Î±, Î¼, g, g_prev = M.Î±, M.Î¼, âˆ‡f(x), M.g_prev
		Î± = Î± + Î¼*(g â‹… g_prev)
		M.g_prev, M.Î± = g, Î±
		return x - Î±*g
	end
end

# â•”â•â•¡ b78278a0-14fb-11eb-0c8b-dd1d29967a94
fâ‚â‚€ = fâ‚„

# â•”â•â•¡ bb6aa8f0-14f3-11eb-2d69-bbc0e5d83f1e
@bind nâ‚â‚€ Slider(1:100, default=22, show_value=true)

# â•”â•â•¡ 365cf2a0-14fb-11eb-1044-779ff3953853
@bind xâ‚â‚€áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ 361e3bf0-14fb-11eb-250b-97be54cd99f7
@bind yâ‚â‚€áµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ 12723860-1504-11eb-0e56-c9d2ed88cd48
md"Let's do the same to Nesterov momentum"

# â•”â•â•¡ 583b4f30-14ff-11eb-251b-ed6cb326bb44
begin
	mutable struct HyperNesterovMomentum <: DescentMethod
		Î±â‚€::Float64 #Initial learning rate
		Î¼::Float64 #Inception learning rate
		Î²::Float64
		v::Array{Float64, 1}
		Î±::Float64
		g_prev::Array{Float64, 1}
	end
	
	HyperNesterovMomentum(Î±â‚€::Number, Î¼::Number, Î²::Number) = HyperNesterovMomentum(Î±â‚€, Î¼, Î², Float64[], Î±â‚€, Float64[])
	
	function init!(M::HyperNesterovMomentum, f, âˆ‡f, x)
		M.Î± = M.Î±â‚€
		M.v = zeros(length(x))
		M.g_prev = zeros(length(x))
		return M
	end
	
	function step!(M::HyperNesterovMomentum, f, âˆ‡f, x)::Array{Float64, 1}
		Î±, Î², v = M.Î±, M.Î², M.v
		g, Î¼, g_prev = âˆ‡f(x), M.Î¼, M.g_prev
		next_g = Î²*v
		Î± = Î± + Î¼*(g â‹… (-g_prev - Î²*v))
		v[:] = Î²*v + g
		M.Î±, M.g_prev = Î±, g_prev
		return x - Î±*(g + Î²*v)
	end
end

# â•”â•â•¡ 0d9a9980-1410-11eb-003d-8dbb0dff6ff4
function optimize!(M::DescentMethod, f, âˆ‡f, x; k=30)::Array{Tuple}
	init!(M, f, âˆ‡f, x)
	points::Array{Tuple} = Array{Tuple}(undef, k+1)
	for i in 1:k
		points[i] = tuple(x...)
		x = step!(M, f, âˆ‡f, x)
	end
	points[end] = tuple(x...)
	points
end

# â•”â•â•¡ 08a33cc0-1410-11eb-1e21-134f56f9a134
begin
	xâ‚‚ = range(-3, stop=3, length=200)
	yâ‚‚ = xâ‚‚
	colorâ‚‚ = cgrad([:grey, :blue])
	âˆ‡fâ‚‚(xÌ„) = Calculus.gradient(fâ‚‚, xÌ„)
	pointsâ‚‚ = [x for x in optimize!(GradientDescent(.052), fâ‚‚, âˆ‡fâ‚‚, [xâ‚‚áµ¢, yâ‚‚áµ¢], k=nâ‚‚)]
	contour(xâ‚‚, yâ‚‚, fâ‚‚, c=colorâ‚‚, levels=700 .^ (range(-.2,stop=1,length=14)))
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚‚áµ¢, yâ‚‚áµ¢), markersize=2, c=:black, label="Initial point")
	plot!(pointsâ‚‚, c=:red, label="Descent")
	scatter!(pointsâ‚‚, c=:green, markersize=1.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ aec7c7a2-1411-11eb-0302-2b383519fc53
md"The optimal point is near _$(pointsâ‚‚[end])_ ğŸ‘€ "

# â•”â•â•¡ f2664400-1411-11eb-378e-dbf7d2ca2597
md"The optimized value is _$(fâ‚‚(pointsâ‚‚[end]))_ ğŸ˜’ğŸ¤¢"

# â•”â•â•¡ abfc14c0-1422-11eb-0746-2375b2a7f411
begin
	xâ‚ƒ = range(-3, stop=3, length=200)
	yâ‚ƒ = xâ‚ƒ
	colorâ‚ƒ = cgrad([:grey, :blue])
	âˆ‡fâ‚ƒ(xÌ„) = Calculus.gradient(fâ‚ƒ, xÌ„)
	pointsâ‚ƒ = [x for x in optimize!(ConjugateGradientDescent(), fâ‚ƒ, âˆ‡fâ‚ƒ, [xâ‚ƒáµ¢, yâ‚ƒáµ¢]; k=nâ‚ƒ)]
	contour(xâ‚ƒ, yâ‚ƒ, fâ‚ƒ, c=colorâ‚ƒ, levels=700 .^ (range(-.2,stop=1,length=14)))
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚ƒáµ¢, yâ‚ƒáµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚ƒ, c=:red, label="Descent")
	scatter!(pointsâ‚ƒ, c=:green, markersize=2.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ e8a44fc2-1425-11eb-1134-e7e1073a68c5
md"The optimal point is near _$(pointsâ‚ƒ[end])_ ğŸŒŸ "

# â•”â•â•¡ 67b3aa80-1427-11eb-2483-51fb1ed66625
md"The optimized value is _$(fâ‚ƒ(pointsâ‚ƒ[end]))_ ğŸ¥³"

# â•”â•â•¡ f6dc15e0-142b-11eb-30fb-718decba0cb3
begin
	xâ‚„ = range(-1.5, stop=1.25, length=200)
	yâ‚„ = range(-.45, stop=1.5, length=200)
	colorâ‚„ = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚„(xÌ„) = Calculus.gradient(fâ‚„, xÌ„)
	pointsâ‚„ = [x for x in optimize!(Momentum(.005, 1), fâ‚„, âˆ‡fâ‚„, [xâ‚„áµ¢, yâ‚„áµ¢]; k=nâ‚„)]
	contour(xâ‚„, yâ‚„, fâ‚„, c=colorâ‚„, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚„áµ¢, yâ‚„áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚„, c=:red, label="Descent")
	scatter!(pointsâ‚„, c=:green, markersize=2.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ 09d749c0-142d-11eb-20d6-49c529dda237
md"The optimal point is near _$(pointsâ‚„[end])_ ğŸ¤·â€â™€ï¸ "

# â•”â•â•¡ 2f0e9220-142d-11eb-23f9-cd8664d7d7f7
md"The optimized value is _$(fâ‚„(pointsâ‚„[end]))_ ğŸ¤¦â€â™‚ï¸"

# â•”â•â•¡ f1b79230-1479-11eb-22d6-c3540c7f6ae1
begin
	xâ‚… = range(-1.5, stop=1.25, length=200)
	yâ‚… = range(-.45, stop=1.5, length=200)
	colorâ‚… = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚…(xÌ„) = Calculus.gradient(fâ‚…, xÌ„)
	pointsâ‚… = [x for x in optimize!(NesterovMomentum(.0009, .95), fâ‚…, âˆ‡fâ‚…, [xâ‚…áµ¢, yâ‚…áµ¢]; k=nâ‚…)]
	contour(xâ‚…, yâ‚…, fâ‚…, c=colorâ‚…, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚…áµ¢, yâ‚…áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚…, c=:red, label="Descent")
	scatter!(pointsâ‚…, c=:green, markersize=1.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ ce3736b0-147b-11eb-3acf-4917b0134727
md"The optimal point is near _$(pointsâ‚…[end])_ ğŸ¤” "

# â•”â•â•¡ d5bd2d90-147b-11eb-3af9-7bfab1d3efe8
md"The optimized value is _$(fâ‚…(pointsâ‚…[end]))_ ğŸ¤”ğŸ¤”"

# â•”â•â•¡ 0660db82-148c-11eb-1823-9f03b67f13fa
begin
	xâ‚† = range(-1.5, stop=1.25, length=200)
	yâ‚† = range(-.45, stop=1.5, length=200)
	colorâ‚† = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚†(xÌ„) = Calculus.gradient(fâ‚†, xÌ„)
	pointsâ‚† = [x for x in optimize!(Adagrad(.5), fâ‚†, âˆ‡fâ‚†, [xâ‚†áµ¢, yâ‚†áµ¢]; k=nâ‚†)]
	contour(xâ‚†, yâ‚†, fâ‚†, c=colorâ‚†, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚†áµ¢, yâ‚†áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚†, c=:red, label="Descent")
	scatter!(pointsâ‚†, c=:green, markersize=1.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ aa77ffa0-148c-11eb-0641-19ad5798aa8d
md"The optimal point is near _$(pointsâ‚†[end])_ ğŸ¤£ "

# â•”â•â•¡ afa20520-148c-11eb-0568-b163d9d760e5
md"The optimized value is _$(fâ‚†(pointsâ‚†[end]))_ ğŸŒğŸŒğŸŒ"

# â•”â•â•¡ e77893d0-148f-11eb-3caf-639d09e446f5
begin
	xâ‚‡ = range(-1.5, stop=1.5, length=200)
	yâ‚‡ = range(-.45, stop=1.5, length=200)
	colorâ‚‡ = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚‡(xÌ„) = Calculus.gradient(fâ‚‡, xÌ„)
	pointsâ‚‡ = [x for x in optimize!(RMSProp(.04, .999, 1), fâ‚‡, âˆ‡fâ‚‡, [xâ‚‡áµ¢, yâ‚‡áµ¢]; k=nâ‚‡)]
	contour(xâ‚‡, yâ‚‡, fâ‚‡, c=colorâ‚‡, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚‡áµ¢, yâ‚‡áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚‡, c=:red, label="Descent")
	scatter!(pointsâ‚‡, c=:green, markersize=1, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ 84527b80-14e0-11eb-0175-ddaa65c37cde
begin
	xâ‚ˆ = range(-1.5, stop=1.5, length=200)
	yâ‚ˆ = range(-.45, stop=1.5, length=200)
	colorâ‚ˆ = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚ˆ(xÌ„) = Calculus.gradient(fâ‚ˆ, xÌ„)
	pointsâ‚ˆ = [x for x in optimize!(Adadelta(.9, .9999, 3e-2), fâ‚ˆ, âˆ‡fâ‚ˆ, [xâ‚ˆáµ¢, yâ‚ˆáµ¢]; k=nâ‚ˆ)]
	contour(xâ‚ˆ, yâ‚ˆ, fâ‚ˆ, c=colorâ‚ˆ, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚ˆáµ¢, yâ‚ˆáµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚ˆ, c=:red, label="Descent")
	scatter!(pointsâ‚ˆ, c=:green, markersize=1, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ ee0e6410-14ec-11eb-3aac-590c727498f8
begin
	xâ‚‰ = range(-1.5, stop=1.5, length=200)
	yâ‚‰ = range(-.45, stop=1.5, length=200)
	colorâ‚‰ = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚‰(xÌ„) = Calculus.gradient(fâ‚‰, xÌ„)
	pointsâ‚‰ = [x for x in optimize!(Adam(.3, .95, .99), fâ‚‰, âˆ‡fâ‚‰, [xâ‚‰áµ¢, yâ‚‰áµ¢]; k=nâ‚‰)]
	contour(xâ‚‰, yâ‚‰, fâ‚‰, c=colorâ‚‰, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚‰áµ¢, yâ‚‰áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚‰, c=:red, label="Descent")
	scatter!(pointsâ‚‰, c=:green, markersize=1.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ 3602c4b0-14fb-11eb-05ee-1fdf72a238a7
begin
	xâ‚â‚€ = range(-1.5, stop=1.5, length=200)
	yâ‚â‚€ = range(-.45, stop=1.5, length=200)
	colorâ‚â‚€ = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚â‚€(xÌ„) = Calculus.gradient(fâ‚â‚€, xÌ„)
	pointsâ‚â‚€ = [x for x in optimize!(HyperGradientDescent(.005, 1e-8), fâ‚â‚€, âˆ‡fâ‚â‚€, [xâ‚â‚€áµ¢, yâ‚â‚€áµ¢]; k=nâ‚â‚€)]
	contour(xâ‚â‚€, yâ‚â‚€, fâ‚â‚€, c=colorâ‚â‚€, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚â‚€áµ¢, yâ‚â‚€áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚â‚€, c=:red, label="Descent")
	scatter!(pointsâ‚â‚€, c=:green, markersize=1.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ 820ea300-1501-11eb-34aa-ef388563baa8
fâ‚â‚ = fâ‚„

# â•”â•â•¡ 43917990-1501-11eb-02f1-45e5d8b91637
@bind nâ‚â‚ Slider(1:200, default=22, show_value=true)

# â•”â•â•¡ 4ad648c0-1501-11eb-3c64-0f6089cd7a4a
@bind xâ‚â‚áµ¢ Slider(-1.5:.01:1.5, default=-0.77, show_value=true)

# â•”â•â•¡ 4e0f53b0-1501-11eb-0ece-15db048a27fd
@bind yâ‚â‚áµ¢ Slider(-.5:.01:1.5, default=0, show_value=true)

# â•”â•â•¡ 4de66ef0-1501-11eb-3ffc-0d2721a515a0
begin
	xâ‚â‚ = range(-1.5, stop=1.5, length=200)
	yâ‚â‚ = range(-.45, stop=1.5, length=200)
	colorâ‚â‚ = cgrad([:yellow,:green, :blue, :red, :purple])
	âˆ‡fâ‚â‚(xÌ„) = Calculus.gradient(fâ‚â‚, xÌ„)
	pointsâ‚â‚ = [x for x in optimize!(HyperNesterovMomentum(.5e-3, 3e-9, .97), fâ‚â‚, âˆ‡fâ‚â‚, [xâ‚â‚áµ¢, yâ‚â‚áµ¢]; k=nâ‚â‚)]
	contour(xâ‚â‚, yâ‚â‚, fâ‚â‚, c=colorâ‚â‚€, levels=2500 .^ (range(-.5,stop=1,length=25)), legend=false)
	scatter!((1.0, 1.0), markersize=3, markerstrokewidth=0, c=:cyan, label="Optimal point")
	scatter!((xâ‚â‚áµ¢, yâ‚â‚áµ¢), markersize=4, c=:black, label="Initial point")
	plot!(pointsâ‚â‚, c=:red, label="Descent")
	scatter!(pointsâ‚â‚, c=:green, markersize=1.5, markerstrokewidth=0, label="Path")
end

# â•”â•â•¡ 7c1abdd0-1501-11eb-2d93-53d722c2d8dc
function plot_method!(M::DescentMethod, f, âˆ‡f, x, k; curve=:red, path=:green)
	methodname = string(nameof(typeof(M)))
	points = [x for x in optimize!(M, f, âˆ‡f, x; k=k)]
	plot!(points, c=curve, label="$(methodname)")
	scatter!(points, c=path, markersize=1, markerstrokewidht=0, label=nothing)
end

# â•”â•â•¡ 4dc886b0-1501-11eb-0c20-cfbbb42f5f72
begin
	methodsâ‚ = [
		(GradientDescent(.001), 			:red),
		(Momentum(2e-3, .5), 				:blue),
		(Adagrad(.1, 1), 					:green),
		(Adadelta(.95, .99, .02),				:cyan),
		(HyperGradientDescent(.002, 1e-8),	:yellow)	
	]
	pâ‚ = contour(
		xâ‚™, yâ‚™, fâ‚™, c=cgrad([:pink,:green, :blue]),
		levels=5000 .^ (range(0,stop=1,length=14)),
		size=(680,400)
	)
	scatter!((xâ‚™áµ¢, yâ‚™áµ¢), markersize=5, markerstrokewidth=0, c=:blue, label="Initial point")
	scatter!((1.0, 1.0), markersize=5, markerstrokewidth=0, c=:red, label="Optimal point")
	for (method, color) in methodsâ‚
		plot_method!(method, fâ‚™, âˆ‡fâ‚™, [xâ‚™áµ¢, yâ‚™áµ¢], nâ‚™, curve=color)		
	end
	pâ‚
end

# â•”â•â•¡ 6dad64f0-154c-11eb-2249-fbc7007af1d7
begin
	methodsâ‚‚ = [
		(ConjugateGradientDescent(), 		:red),
		(NesterovMomentum(.0002, .95), 	:purple),
		(RMSProp(.01, .6), 					:black),
		(Adam(.35, .9, .999),				:green)
	]
	pâ‚‚ = contour(
		xâ‚™, yâ‚™, fâ‚™, c=cgrad([:pink,:green, :blue]),
		levels=5000 .^ (range(0,stop=1,length=14)),
		size=(680,400)
	)
	scatter!((xâ‚™áµ¢, yâ‚™áµ¢), markersize=5, markerstrokewidth=0, c=:blue, label="Initial point")
	scatter!((1.0, 1.0), markersize=5, markerstrokewidth=0, c=:red, label="Optimal point")
	for (method, color) in methodsâ‚‚
		plot_method!(method, fâ‚™, âˆ‡fâ‚™, [xâ‚™áµ¢, yâ‚™áµ¢], nâ‚™, curve=color)		
	end
	pâ‚‚
end

# â•”â•â•¡ 95e9f9e0-1553-11eb-25ee-5ddaf4e0e246
md"## Exercises"

# â•”â•â•¡ 9b7ea810-1553-11eb-0b1a-f300b8082254
md"**Exercise 5.1.** Compute the gradient of \$\$x^TAx+b^Tx\$\$ when A is symmetric"

# â•”â•â•¡ 49755460-155d-11eb-2013-7367b16844e2
md"**Answer**: By decomposing the matrix as a sum (really long) and then replacing the sum as a matrix form we get: \$\$âˆ‡f=2Ax+b\$\$"

# â•”â•â•¡ 4fa175ae-155f-11eb-20c9-412e7896925b
md"**Exercise 5.2.** Apply gradient descent with unit step to f(x)=xâ´, compute two iterations."

# â•”â•â•¡ 72138390-155f-11eb-0947-634e85008971
begin
	fâ‚‘(x::Float64)::Array{Float64, 1} = [x^4]
	fâ‚‘(xÌ„::Array{Float64, 1})::Array{Float64, 1} = fâ‚‘(xÌ„[1])
	âˆ‡fâ‚‘(x::Float64)::Array{Float64, 1} = [4*x^3]
	âˆ‡fâ‚‘(xÌ„::Array{Float64, 1})::Array{Float64, 1} = âˆ‡fâ‚‘(xÌ„[1])
end

# â•”â•â•¡ b6ff677e-155f-11eb-23fb-67b742123128
begin
	xâ‚‘ = range(-.01, .5, length=50)
	plot(xâ‚‘, (x) -> fâ‚‘(x)[1], label="Function")
	pointsâ‚‘ = [x[1] for x in optimize!(GradientDescent(1), fâ‚‘, âˆ‡fâ‚‘, [.4]; k=2)]
	plot!(pointsâ‚‘, (x) -> fâ‚‘(x)[1], c=:red, label="Gradient descent")
	scatter!(pointsâ‚‘, (x) -> fâ‚‘(x)[1], c=:black, markersize=1, markerstrokewidht=0, label=nothing)
end

# â•”â•â•¡ 2f41ecc0-1562-11eb-1411-653c8c741c5d
md"**Exercise 5.3.** Apply one step of gradient descent to f(x)=e^x+e^-x from x =10 with both a unit step and with exact line search."

# â•”â•â•¡ 58840f00-1562-11eb-0c1a-594bdf2fd540
begin
	fâ‚˜(x::Float64)::Array{Float64, 1} = [exp(x)+exp(-x)]
	fâ‚˜(xÌ„::Array{Float64, 1})::Array{Float64, 1} = fâ‚˜(xÌ„[1])
	âˆ‡fâ‚˜(x::Float64)::Array{Float64, 1} = Calculus.derivative(fâ‚˜, x)
	âˆ‡fâ‚˜(xÌ„::Array{Float64, 1})::Array{Float64, 1} = âˆ‡fâ‚˜(xÌ„[1])
end

# â•”â•â•¡ 7ce1ffa0-1568-11eb-26c0-9f5828fd6f13
md"**Answer**: If someone can do this without the gradient exploting to infinity really quick please help."

# â•”â•â•¡ 9275d52e-1568-11eb-3289-0563674bb57a
md"**Exercise 5.4.** The conjugate gradient method can be used to find a search direction d when a local quadratic model of a function is available at the current point. With d as search direction, let the model be \$\$q(d) = d^T Hd+b^Td+c\$\$ for a symmetric matrix H, What is the Hessian in this case? What is the gradient of q when d=0? What can go wrong if the conjugate gradient method is applied to the quadratic model to get the search direction d?"

# â•”â•â•¡ 8b9577b0-1569-11eb-0cc3-e3cf00f2176f
md"**Answer**: We knew that: \$\$âˆ‡q = 2Hd\$\$, then the gradient (gradient of gradient) will be: \$\$âˆ‡^2q=2H\$\$"

# â•”â•â•¡ db7d0660-156b-11eb-2e2d-03747a944969
md"When d = 0 the gradient will also be 0. And if this happens we will be dividing by infinitesimal values when computing the next search direction. (This happened to me when using too many iterations)"

# â•”â•â•¡ 694e9120-156c-11eb-017e-db4890cc517a
md"**Exercise 5.5.** How is Nesterov momentum an improvement over momentum?"

# â•”â•â•¡ 786ca74e-156c-11eb-079c-639446c1339e
md"**Answer**: The problem with momentum is that it doesn't slow down, and Nesterov momentum uses the calculation of the gradient at the next step (if it overshoots the gradient will make it go back, correcting the step)."

# â•”â•â•¡ dd160b00-156d-11eb-2aa3-6d96c0546927
md"**Exercise 5.6.** In what way is the conjugate gradient method an improvement over steepest descent?"

# â•”â•â•¡ f095f4b0-156d-11eb-3082-754f947b8589
md"**Answer**: Gradient descent is really slow near valleys (because the next direction will always be orthogonal to the previous, causing a zig-zag movement), that is corrected when the previous gradient contributes to the next direction (making it more like a straight path to the minimum)."

# â•”â•â•¡ a6659e10-1570-11eb-27fa-fd58fa23e5ef
md"**Exercise 5.7.** In the conjugate gradient descent, what is the normalized descent direction at the first iteration for the function \$\$f(x, y) = x^2+xy+y^2+5\$\$ when initialized at (x,y) = (1, 1)? What is the resulting point after two steps of the conjugate gradient method?"

# â•”â•â•¡ f3f9ffde-1570-11eb-0f66-c143d09887b2
begin
	fâ‚š(x, y) = x^2 + x*y + y^2 + 5
	fâ‚š(xÌ„) = fâ‚š(xÌ„...)
	âˆ‡fâ‚š(xÌ„) = Calculus.gradient(fâ‚š, xÌ„)
end

# â•”â•â•¡ e2ef7440-1576-11eb-21ad-257f12fb8cef
begin
	plotly()
	xâ‚š = range(-3, 3, length=40)
	yâ‚š = range(-3, 3, length=40)
	pâ‚š = contour(xâ‚š, yâ‚š, fâ‚š)
	plot_method!(ConjugateGradientDescent(), fâ‚š, âˆ‡fâ‚š, [1, 1], 2)
	gr()
	pâ‚š
end

# â•”â•â•¡ 9e3ea810-1577-11eb-3775-2952691b39fa
begin
	Mâ‚š = ConjugateGradientDescent()
	init!(Mâ‚š, fâ‚š, âˆ‡fâ‚š, [1, 1])
	xâ‚šâ‚€ = [1, 1]
	xâ‚šâ‚ = step!(Mâ‚š, fâ‚š, âˆ‡fâ‚š, xâ‚šâ‚€)
	râ‚, râ‚‚ = Mâ‚š.d[1], Mâ‚š.d[2]
	xâ‚šâ‚‚ = step!(Mâ‚š, fâ‚š, âˆ‡fâ‚š, xâ‚šâ‚)
end;

# â•”â•â•¡ 88e93b10-1577-11eb-23ba-17240069d759
md"**Answer**: The direction is [_$(râ‚)_, _$(râ‚‚)_], (Almost the same on each component), And the point is [_$(xâ‚šâ‚‚[1])_, _$(xâ‚šâ‚‚[2])_] after two iterations (expected because this function is polinomial of order two and the conjugate gradient should optimize it in two steps)."

# â•”â•â•¡ 0a855b30-1579-11eb-3250-55615f863cf2
md"**Exercise 5.8.** We have a polynomial function f such that f(x) > 2 for all x in three-dimensional Euclidean space. Suppose we are using steepest descent with step lengths optimized at each step, and we want to find a local minimum of f. If our unnormalized descent direction is [1,2,3] at step k, is it possible for our unnormalized descent direction at step k+1 to be [0,0,-3]? Why or why not?"

# â•”â•â•¡ 54c20680-1579-11eb-2ca3-a55b5a96a439
md"**Answer**: We know that in the steepest descent method the descent direction in every iteration is orthogonal to the next direction. \$\$[1,2,3]â‹…[0,0,3] â‰  0\$\$ Then the unnormalized descent direction at step k+1 cannot be [0,0,-3]. (TODO: Check the restriction for f(x) > 2)"

# â•”â•â•¡ Cell order:
# â•Ÿâ”€8bd329d0-13f1-11eb-0e83-25a8a9620553
# â•Ÿâ”€68f68ae0-1510-11eb-0bb6-bb84b68d18e6
# â•Ÿâ”€a97881d0-150c-11eb-3391-b52524901b4c
# â•Ÿâ”€c71545d0-154c-11eb-38d2-ebdde946ae88
# â•Ÿâ”€fbe594ce-150c-11eb-3135-fb523622c0cd
# â•Ÿâ”€fbc03280-150c-11eb-3400-cf19d15006ba
# â•Ÿâ”€fb16ac10-150c-11eb-23af-816bfbad1ad9
# â•Ÿâ”€4dc886b0-1501-11eb-0c20-cfbbb42f5f72
# â•Ÿâ”€6dad64f0-154c-11eb-2249-fbc7007af1d7
# â•Ÿâ”€9cf0b980-13f1-11eb-2274-6bc775d469c8
# â• â•839a6840-13f2-11eb-3ee7-3b513eb25cac
# â•Ÿâ”€cb1de700-13f2-11eb-2ace-afc2e372b013
# â•Ÿâ”€ea5c8f40-13f2-11eb-22b5-61728c0e4388
# â•Ÿâ”€290a8cb0-13f3-11eb-2069-4f8e02d036a8
# â• â•3a625020-13fc-11eb-3cbb-95bafc7c95f9
# â•Ÿâ”€bcfb7970-13f3-11eb-3e89-d7e04bfc65ad
# â• â•58ee91f0-13f4-11eb-1d27-31633b18d5ee
# â• â•246f8b50-13f4-11eb-2e29-977facb5c488
# â• â•33062200-13f4-11eb-21f1-1ffa95638977
# â• â•d330cce2-13f8-11eb-2d7b-eb0338c3d1b8
# â• â•dcdfefe0-13f9-11eb-337a-2d8a80e4a5e7
# â• â•febfa170-13fc-11eb-3291-f9bdc4e9f971
# â• â•dd193610-13fe-11eb-1724-77e60a1eb306
# â•Ÿâ”€ecdf9a32-13fe-11eb-3ff9-5166ac683584
# â•Ÿâ”€6dc753e0-13ff-11eb-01ae-7d4477726252
# â•Ÿâ”€9841a800-13ff-11eb-1ef2-0b366a3d3b22
# â• â•01b03910-13fa-11eb-38fe-7ff1876faefc
# â•Ÿâ”€8107d7b0-140c-11eb-0194-67b53124e6a2
# â•Ÿâ”€965666e0-140c-11eb-3d64-6fea80cb3edc
# â• â•8116c470-140f-11eb-1b12-230e8c180dfe
# â• â•2fdb8310-1401-11eb-1529-ed74dd276380
# â• â•a8ae43ee-140f-11eb-2adf-4f4e1d0a8ef5
# â• â•7cc17412-140f-11eb-0411-4f7deaa61fae
# â• â•0d9a9980-1410-11eb-003d-8dbb0dff6ff4
# â• â•bd87d4c2-1410-11eb-3a2c-07be02f2d049
# â•Ÿâ”€df37f230-1410-11eb-0593-d5008a92eae1
# â•Ÿâ”€dea91fb0-1410-11eb-2c52-836b5e46eb28
# â•Ÿâ”€de622ba0-1410-11eb-2910-b1e14a97878f
# â•Ÿâ”€08a33cc0-1410-11eb-1e21-134f56f9a134
# â•Ÿâ”€aec7c7a2-1411-11eb-0302-2b383519fc53
# â•Ÿâ”€f2664400-1411-11eb-378e-dbf7d2ca2597
# â•Ÿâ”€fa788512-1400-11eb-286c-85633d52af21
# â•Ÿâ”€d1732fa0-13fe-11eb-39f0-670f5eaa94ec
# â•Ÿâ”€4d0090c0-141a-11eb-3bbe-d9049235aca1
# â•Ÿâ”€52541a70-141e-11eb-2096-c75cb6bfd792
# â•Ÿâ”€5d834dc0-141f-11eb-25eb-63933ff6ae4c
# â•Ÿâ”€0bbda8e0-1420-11eb-25bc-c34eaf97c1b6
# â•Ÿâ”€2418b4c0-1420-11eb-1d36-b5c012477a99
# â•Ÿâ”€75156ee0-1420-11eb-21ba-71bbcff2a1ed
# â•Ÿâ”€9a938440-1420-11eb-19f5-91f76c2eab12
# â•Ÿâ”€0d96b92e-1421-11eb-11f0-6dc604eeb10a
# â•Ÿâ”€40b8c7e0-1421-11eb-3e76-5db9f1885668
# â• â•744aaf60-1421-11eb-0dbd-1fd4f4775c8d
# â• â•08462370-1422-11eb-1ea0-ef8e58d85970
# â• â•49266a30-1422-11eb-1d84-9d24a1b81e04
# â• â•98c33ba0-1421-11eb-23ce-a76943bc3a76
# â• â•8accdfc0-1425-11eb-18fb-29f87b39e006
# â•Ÿâ”€6bcaf260-1425-11eb-1b6a-b14640cd9d64
# â•Ÿâ”€7163e650-1425-11eb-2bb9-6b6b8c3bf71a
# â•Ÿâ”€7ac3cd50-1425-11eb-1a93-adaeb3dac3f4
# â•Ÿâ”€abfc14c0-1422-11eb-0746-2375b2a7f411
# â•Ÿâ”€e8a44fc2-1425-11eb-1134-e7e1073a68c5
# â•Ÿâ”€67b3aa80-1427-11eb-2483-51fb1ed66625
# â•Ÿâ”€7d0d8450-1427-11eb-373d-fd77eca043b3
# â•Ÿâ”€b1426290-1427-11eb-3ff3-4d936d73df78
# â•Ÿâ”€d250f360-1428-11eb-3067-7bf3b10eeb95
# â•Ÿâ”€bf228510-1428-11eb-35e0-ef99d695c0de
# â• â•ff655350-1428-11eb-2baf-079bacea6696
# â• â•b9c8eea0-1429-11eb-315c-316fc17a30a1
# â• â•0c382480-142a-11eb-026a-77a06e8eb9d0
# â• â•87a82de0-142a-11eb-3c40-3f50754cc0fe
# â•Ÿâ”€1b6bfab0-142c-11eb-1d7e-995795b8d26f
# â•Ÿâ”€2279db60-142c-11eb-1d5d-f16fbfdf7451
# â•Ÿâ”€23190190-142c-11eb-2cc2-614ee78d3e8a
# â• â•f6dc15e0-142b-11eb-30fb-718decba0cb3
# â•Ÿâ”€09d749c0-142d-11eb-20d6-49c529dda237
# â•Ÿâ”€2f0e9220-142d-11eb-23f9-cd8664d7d7f7
# â•Ÿâ”€349bc640-142d-11eb-3baa-e7dc46b4b8a1
# â•Ÿâ”€76d33890-142d-11eb-253a-491b80114239
# â•Ÿâ”€a38f6660-142d-11eb-21a2-8529e11f8acb
# â•Ÿâ”€9b6ad3a0-142f-11eb-3482-ed377d4bb2a8
# â•Ÿâ”€72a17620-1477-11eb-166a-fd7dac2a01bd
# â• â•93479da0-1477-11eb-0c7b-a9446b9dd6d3
# â• â•f8d6b0f0-1479-11eb-1c27-4b98bb05bd34
# â• â•fd7f9f92-1479-11eb-11de-c9b923f3fe06
# â• â•0424d360-147a-11eb-2b4a-99845e4f9e5a
# â• â•0bc30d30-147a-11eb-2480-53595b44dd79
# â• â•f1b79230-1479-11eb-22d6-c3540c7f6ae1
# â•Ÿâ”€ce3736b0-147b-11eb-3acf-4917b0134727
# â•Ÿâ”€d5bd2d90-147b-11eb-3af9-7bfab1d3efe8
# â•Ÿâ”€db2fd4d0-147b-11eb-35a8-770858111cba
# â•Ÿâ”€0c426470-147c-11eb-1e91-f131c1a1da0b
# â•Ÿâ”€ad146fb0-147c-11eb-3756-33ad44898b84
# â•Ÿâ”€cbecf130-147e-11eb-0748-0d4f4af1775c
# â•Ÿâ”€e572c170-147e-11eb-0931-c5485d3e4203
# â•Ÿâ”€349f9d90-147f-11eb-3868-8b295d0f4e59
# â•Ÿâ”€49aaba22-148a-11eb-3782-cd3dcb9a53b1
# â•Ÿâ”€6a12c730-148a-11eb-3fa7-63cf835a531d
# â•Ÿâ”€7d457b40-148a-11eb-101a-15c60623e966
# â• â•af9daa40-148a-11eb-0b19-452c71a11b26
# â• â•e6fe3080-148b-11eb-28ba-8f5a1b03e5cb
# â•Ÿâ”€f62399b0-148b-11eb-2078-c97839d84aaf
# â•Ÿâ”€fb8c07c0-148b-11eb-3291-f985249cb0ae
# â•Ÿâ”€00b019d0-148c-11eb-0c2b-bb5287624389
# â•Ÿâ”€0660db82-148c-11eb-1823-9f03b67f13fa
# â•Ÿâ”€aa77ffa0-148c-11eb-0641-19ad5798aa8d
# â•Ÿâ”€afa20520-148c-11eb-0568-b163d9d760e5
# â•Ÿâ”€cf5ede10-148c-11eb-2fe9-ddedbf608aad
# â•Ÿâ”€c6ffcfd2-148d-11eb-3486-1fd4742afaa0
# â•Ÿâ”€e83f7830-148d-11eb-0119-770094c7ad55
# â•Ÿâ”€40e2a980-148e-11eb-1076-5767cba6fff6
# â•Ÿâ”€fc7607ae-148d-11eb-25fa-59a3d2c4c722
# â•Ÿâ”€e5daad30-148d-11eb-224a-1bb3c3d5b20a
# â•Ÿâ”€6595a6b0-148e-11eb-3a6a-8dfdb2ef7933
# â• â•65ad4d5e-148e-11eb-1121-c139c1000455
# â• â•f373bf20-148f-11eb-178b-ddd8ed8df869
# â•Ÿâ”€f71a8000-148f-11eb-08bb-911cfaa79535
# â•Ÿâ”€f87c8100-148f-11eb-3938-2dd7affdd0a4
# â•Ÿâ”€f8981f50-148f-11eb-1b9b-3dfd2d711494
# â• â•e77893d0-148f-11eb-3caf-639d09e446f5
# â•Ÿâ”€0f715b40-1492-11eb-0c2f-4d5ff35414b6
# â•Ÿâ”€17b8a650-1492-11eb-3fcb-9dce6ad8e093
# â•Ÿâ”€0ceaa960-14df-11eb-3e04-613396d49c39
# â• â•35045130-14df-11eb-227e-af37a738cf78
# â• â•8ec902a0-14e0-11eb-0cba-f1917748a92f
# â• â•b429cc00-14e0-11eb-27ba-b76917406f92
# â•Ÿâ”€b3fc0540-14e0-11eb-1975-47351cf57db3
# â•Ÿâ”€b37dfba0-14e0-11eb-161e-812bbfff5a45
# â• â•84527b80-14e0-11eb-0175-ddaa65c37cde
# â•Ÿâ”€22447bd0-14e2-11eb-2385-afe72e00c514
# â•Ÿâ”€8d785802-14e6-11eb-2768-7d57bb0c552d
# â•Ÿâ”€80d5c920-14e6-11eb-0b2b-aba1f51d4c40
# â•Ÿâ”€f4919fb0-14e6-11eb-37da-7f8b001f912f
# â•Ÿâ”€53fc59e0-14e7-11eb-1e42-c93841a25d8b
# â•Ÿâ”€751b81f0-14e7-11eb-30dd-d914bd2aae92
# â•Ÿâ”€75af8490-14e7-11eb-266b-2f84b10c59b6
# â•Ÿâ”€75cb9810-14e7-11eb-0a01-15c658df3cd6
# â•Ÿâ”€75e93230-14e7-11eb-1436-155363e20db8
# â•Ÿâ”€a6b66980-14e9-11eb-1f40-4b6f60bd56db
# â• â•c9666ac2-14e9-11eb-3f14-cbf7adf802d9
# â• â•cf904cb0-14ec-11eb-3cec-af99445c4bb3
# â• â•3955ac20-14ee-11eb-314d-ab74464eb64b
# â•Ÿâ”€393c57c0-14ee-11eb-09fd-a51eb1d06292
# â•Ÿâ”€39257460-14ee-11eb-365d-45eca91e680e
# â• â•ee0e6410-14ec-11eb-3aac-590c727498f8
# â•Ÿâ”€94087c30-14f1-11eb-2df8-27240ebdd2ae
# â•Ÿâ”€3b6a5e82-14f2-11eb-1962-d19f98382b7a
# â•Ÿâ”€5fd33350-14f2-11eb-0548-7db92e9c8551
# â• â•59a20190-14f3-11eb-3b9b-7d4284e3f759
# â• â•b78278a0-14fb-11eb-0c8b-dd1d29967a94
# â•Ÿâ”€bb6aa8f0-14f3-11eb-2d69-bbc0e5d83f1e
# â•Ÿâ”€365cf2a0-14fb-11eb-1044-779ff3953853
# â•Ÿâ”€361e3bf0-14fb-11eb-250b-97be54cd99f7
# â• â•3602c4b0-14fb-11eb-05ee-1fdf72a238a7
# â•Ÿâ”€12723860-1504-11eb-0e56-c9d2ed88cd48
# â• â•583b4f30-14ff-11eb-251b-ed6cb326bb44
# â• â•820ea300-1501-11eb-34aa-ef388563baa8
# â•Ÿâ”€43917990-1501-11eb-02f1-45e5d8b91637
# â•Ÿâ”€4ad648c0-1501-11eb-3c64-0f6089cd7a4a
# â•Ÿâ”€4e0f53b0-1501-11eb-0ece-15db048a27fd
# â• â•4de66ef0-1501-11eb-3ffc-0d2721a515a0
# â• â•7c1abdd0-1501-11eb-2d93-53d722c2d8dc
# â•Ÿâ”€95e9f9e0-1553-11eb-25ee-5ddaf4e0e246
# â•Ÿâ”€9b7ea810-1553-11eb-0b1a-f300b8082254
# â•Ÿâ”€49755460-155d-11eb-2013-7367b16844e2
# â•Ÿâ”€4fa175ae-155f-11eb-20c9-412e7896925b
# â• â•72138390-155f-11eb-0947-634e85008971
# â• â•b6ff677e-155f-11eb-23fb-67b742123128
# â•Ÿâ”€2f41ecc0-1562-11eb-1411-653c8c741c5d
# â• â•58840f00-1562-11eb-0c1a-594bdf2fd540
# â•Ÿâ”€7ce1ffa0-1568-11eb-26c0-9f5828fd6f13
# â•Ÿâ”€9275d52e-1568-11eb-3289-0563674bb57a
# â•Ÿâ”€8b9577b0-1569-11eb-0cc3-e3cf00f2176f
# â•Ÿâ”€db7d0660-156b-11eb-2e2d-03747a944969
# â•Ÿâ”€694e9120-156c-11eb-017e-db4890cc517a
# â•Ÿâ”€786ca74e-156c-11eb-079c-639446c1339e
# â•Ÿâ”€dd160b00-156d-11eb-2aa3-6d96c0546927
# â•Ÿâ”€f095f4b0-156d-11eb-3082-754f947b8589
# â•Ÿâ”€a6659e10-1570-11eb-27fa-fd58fa23e5ef
# â• â•f3f9ffde-1570-11eb-0f66-c143d09887b2
# â• â•e2ef7440-1576-11eb-21ad-257f12fb8cef
# â• â•9e3ea810-1577-11eb-3775-2952691b39fa
# â•Ÿâ”€88e93b10-1577-11eb-23ba-17240069d759
# â•Ÿâ”€0a855b30-1579-11eb-3250-55615f863cf2
# â•Ÿâ”€54c20680-1579-11eb-2ca3-a55b5a96a439
